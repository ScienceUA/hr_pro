import time
import logging
from typing import Optional, Dict, Any
from dataclasses import dataclass

from app.transport.fetcher import SmartFetcher
from app.parsing.base import BaseParser
from app.parsing.serp import SerpParser
from app.parsing.resume import ResumeParser
from app.parsing.models import PageType, DataQuality, ResumePreviewData
from app.storage.repository import JsonlRepository
from app.services.url_builder import UrlBuilder

logger = logging.getLogger(__name__)

@dataclass
class CrawlStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞."""
    pages_processed: int = 0
    candidates_found: int = 0
    candidates_new: int = 0
    candidates_saved: int = 0
    errors_serp: int = 0
    errors_detail: int = 0
    critical_stop: bool = False
    stop_reason: Optional[str] = None

class CrawlerService:
    """
    –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.
    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ü–∏–∫–ª: SERP -> Links -> Dedup -> Detail -> Save.
    –°—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–µ—Ç –∑–∞–¥–µ—Ä–∂–∫–∏ –∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞—Ö.
    """
    
    # –ó–∞–¥–µ—Ä–∂–∫–∏ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö) –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞
    DELAY_SERP = 3.0    # –ú–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ø–∏—Å–∫–∞
    DELAY_DETAIL = 1.5  # –ú–µ–∂–¥—É —Ä–µ–∑—é–º–µ

    def __init__(self, fetcher: SmartFetcher, repository: JsonlRepository):
        self.fetcher = fetcher
        self.repository = repository
        self.stats = CrawlStats()

    def run(
        self,
        query: str,
        city: str = "",
        params: Optional[Dict[str, Any]] = None,
        max_pages: Optional[int] = None,
    ) -> CrawlStats:

        """
        –ó–∞–ø—É—Å–∫ –∫—Ä–∞—É–ª–µ—Ä–∞ –ø–æ –ø–æ–∏—Å–∫–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É.
        """
        # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ URL (—Å —É—á–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
        try:
            start_url = UrlBuilder.build(query, city, params)
        except Exception as e:
            logger.error(f"Failed to build URL: {e}")
            self.stats.critical_stop = True
            self.stats.stop_reason = "URL Build Error"
            return self.stats

        logger.info(f"üöÄ Starting crawl. Query: '{query}', City: '{city}'. URL: {start_url}")
        
        current_url = start_url
        self.stats = CrawlStats()

        while current_url and (max_pages is None or self.stats.pages_processed < max_pages):
            if self.stats.critical_stop:
                break

            logger.info(f"üìÇ Processing SERP page {self.stats.pages_processed + 1}: {current_url}")
            
            # --- 1. Fetching ---
            try:
                html_content = self.fetcher.get(current_url)
                if not html_content:
                    logger.error("Empty response from fetcher for SERP.")
                    self.stats.errors_serp += 1
                    break
            except Exception as e:
                logger.error(f"Network error fetching SERP: {e}")
                self.stats.errors_serp += 1
                break

            # --- 2. Safety Check (Base Parser) ---
            # –°–æ–∑–¥–∞–µ–º –ª–µ–≥–∫–∏–π –ø–∞—Ä—Å–µ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            base_parser = BaseParser(html_content, current_url)
            
            if not self._check_page_safety(base_parser.page_type, context="SERP"):
                break

            if base_parser.page_type != PageType.SERP:
                logger.warning(f"Unexpected page type for SERP: {base_parser.page_type}. Stopping.")
                self.stats.stop_reason = f"Unexpected PageType: {base_parser.page_type}"
                break

            # --- 3. Parsing (Serp Parser) ---
            # –ü–µ—Ä–µ–¥–∞–µ–º –¢–û–¢ –ñ–ï html_content –Ω–∞–ø—Ä—è–º—É—é
            serp_parser = SerpParser(html_content, current_url)
            serp_result = serp_parser.parse()

            if serp_result.quality == DataQuality.ERROR:
                logger.error("Failed to parse SERP structure.")
                self.stats.errors_serp += 1
                break

            previews = serp_result.payload or []
            self.stats.candidates_found += len(previews)
            logger.info(f"   Found {len(previews)} candidates on page.")

            # --- 4. Detail Loop (Candidates) ---
            for preview in previews:
                if self.stats.critical_stop:
                    break
                self._process_candidate(preview)

            # --- 5. Pagination ---
            next_url = serp_result.next_page_url
            
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è (–µ—Å–ª–∏ next_url –≤–µ–¥–µ—Ç –Ω–∞ —Ç—É –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
            if next_url == current_url:
                logger.warning("Next page URL matches current. Loop detected.")
                break
            
            current_url = next_url
            self.stats.pages_processed += 1

            if current_url:
                logger.debug(f"üí§ Sleeping {self.DELAY_SERP}s before next page...")
                time.sleep(self.DELAY_SERP)
        
        logger.info(f"üèÅ Crawl finished. Stats: {self.stats}")
        return self.stats
    
    def preview(self, search_payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        Phase 1 (Preview/Count):
        - –ù–ï —Å–∫–∞—á–∏–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ —Ä–µ–∑—é–º–µ
        - –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç JSONL
        - –¢–æ–ª—å–∫–æ: –ø—Ä–æ—Ö–æ–¥–∏—Ç SERP –ø–∞–≥–∏–Ω–∞—Ü–∏—é, —Å–æ–±–∏—Ä–∞–µ—Ç URL —Ä–µ–∑—é–º–µ —Å–≤–µ—Ä—Ö—É –≤–Ω–∏–∑
        –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç {total_found, urls}.
        """
        query = str(search_payload.get("query") or "").strip()
        city = str(search_payload.get("city") or "").strip()
        params = search_payload.get("params") or {}
        max_pages = search_payload.get("max_pages", None)

        if not query:
            return {"total_found": 0, "urls": []}

        try:
            start_url = UrlBuilder.build(query, city, params)
        except Exception as e:
            logger.error(f"Failed to build URL (preview): {e}")
            return {"total_found": 0, "urls": []}

        current_url = start_url
        pages_processed = 0

        urls: list[str] = []
        seen: set[str] = set()

        while current_url and (max_pages is None or pages_processed < int(max_pages)):
            logger.info(f"üìé Preview SERP page {pages_processed + 1}: {current_url}")

            # --- Fetch ---
            try:
                html_content = self.fetcher.get(current_url)
                if not html_content:
                    logger.error("Empty response from fetcher for SERP (preview).")
                    break
            except Exception as e:
                logger.error(f"Network error fetching SERP (preview): {e}")
                break

            # --- Safety ---
            base_parser = BaseParser(html_content, current_url)
            if not self._check_page_safety(base_parser.page_type, context="SERP_PREVIEW"):
                break
            if base_parser.page_type != PageType.SERP:
                logger.warning(f"Unexpected page type for SERP preview: {base_parser.page_type}. Stopping.")
                break

            # --- Parse SERP ---
            serp_parser = SerpParser(html_content, current_url)
            serp_result = serp_parser.parse()
            if serp_result.quality == DataQuality.ERROR:
                logger.error("Failed to parse SERP structure (preview).")
                break

            previews = serp_result.payload or []
            for p in previews:
                u = getattr(p, "url", None)
                if isinstance(u, str) and u and u not in seen:
                    seen.add(u)
                    urls.append(u)

            next_url = serp_result.next_page_url
            if next_url == current_url:
                logger.warning("Next page URL matches current (preview). Loop detected.")
                break

            current_url = next_url
            pages_processed += 1

            if current_url:
                time.sleep(self.DELAY_SERP)

        # –ï—Å–ª–∏ SerpParser —Å–º–æ–≥ –≤—ã—Ç–∞—â–∏—Ç—å —Ä–µ–∞–ª—å–Ω–æ–µ total_found ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ.
        # –ò–Ω–∞—á–µ fallback: len(urls)
        real_total = None
        try:
            real_total = getattr(serp_result, "total_found", None)
        except Exception:
            real_total = None

        return {"total_found": int(real_total) if isinstance(real_total, int) and real_total > 0 else len(urls), "urls": urls}

    def run_from_urls(self, urls: list[str], out: str) -> str:
        """
        Phase 2 (Full crawl by URLs):
        - —Å–∫–∞—á–∏–≤–∞–µ—Ç –¢–û–õ–¨–ö–û –¥–µ—Ç–∞–ª–∏ –ø–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º URL
        - –ø–∞—Ä—Å–∏—Ç ResumeParser
        - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–µ—Ä–µ–∑ repository.save_result()
        - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å out (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å run_agent.py)
        """
        # –ú—ã –Ω–µ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º repository –∑–¥–µ—Å—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω —É–∂–µ —Å–æ–∑–¥–∞–Ω —Å –Ω—É–∂–Ω—ã–º out_path
        # –≤ load_crawler_service(out_path). out –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –∫–æ–Ω—Ç—Ä–æ–ª—å/–ª–æ–≥.
        if not urls:
            return out

        self.stats = CrawlStats()

        for url in urls:
            if self.stats.critical_stop:
                break

            time.sleep(self.DELAY_DETAIL)

            try:
                html_content = self.fetcher.get(url)
                if not html_content:
                    logger.warning(f"Empty content for detail {url}")
                    self.stats.errors_detail += 1
                    continue
            except Exception as e:
                logger.error(f"Failed to fetch detail {url}: {e}")
                self.stats.errors_detail += 1
                continue

            base_parser = BaseParser(html_content, url)

            if base_parser.page_type == PageType.NOT_FOUND:
                logger.warning(f"Resume not found (404): {url}")
                continue

            if not self._check_page_safety(base_parser.page_type, context="DETAIL"):
                break

            resume_parser = ResumeParser(html_content, url)
            result = resume_parser.parse()

            if result.quality == DataQuality.ERROR:
                logger.warning(f"Parser Error for {url}: {result.error_message}")
                self.stats.errors_detail += 1
                continue

            try:
                self.repository.save_result(result)
                self.stats.candidates_saved += 1

                candidate_name = result.payload.name if result.payload else "Unknown"
                candidate_title = result.payload.title if result.payload else "Unknown Title"
                logger.info(f"‚úÖ Saved: {candidate_name} ({candidate_title})")

            except Exception as e:
                logger.error(f"Failed to save result for {url}: {e}")
                self.stats.errors_detail += 1

        logger.info(f"üèÅ run_from_urls finished. Stats: {self.stats}")
        return out

    
    def _process_candidate(self, preview: ResumePreviewData):
        """
        –õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è -> –°–∫–∞—á–∏–≤–∞–Ω–∏–µ -> –ü–∞—Ä—Å–∏–Ω–≥ -> –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ.
        """
        # 1. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (In-Memory check)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ resume_id (–∫–ª—é—á–µ–≤–æ–π –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç)
        if self.repository.exists(preview.resume_id):
            logger.debug(f"   Skipping existing ID: {preview.resume_id}")
            return

        self.stats.candidates_new += 1
        
        # 2. Throttling –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º –¥–µ—Ç–∞–ª–∫–∏
        time.sleep(self.DELAY_DETAIL)

        # 3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            html_content = self.fetcher.get(preview.url)
            if not html_content:
                logger.warning(f"   Empty content for detail {preview.url}")
                self.stats.errors_detail += 1
                return
        except Exception as e:
            logger.error(f"   Failed to fetch detail {preview.url}: {e}")
            self.stats.errors_detail += 1
            return

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ (Safety Checks)
        base_parser = BaseParser(html_content, preview.url)
        
        # –ï—Å–ª–∏ 404 - —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        if base_parser.page_type == PageType.NOT_FOUND:
            logger.warning(f"   Resume not found (404): {preview.url}")
            return
            
        # –ï—Å–ª–∏ –ë–∞–Ω/–ö–∞–ø—á–∞ - —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –≤—Å–µ–π —Å–µ—Å—Å–∏–∏
        if not self._check_page_safety(base_parser.page_type, context="DETAIL"):
            return

        # 5. –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        resume_parser = ResumeParser(html_content, preview.url)
        result = resume_parser.parse()

        if result.quality == DataQuality.ERROR:
            logger.warning(f"   Parser Error for {preview.resume_id}: {result.error_message}")
            self.stats.errors_detail += 1
            return

        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è save_result(ParsingResult)
            self.repository.save_result(result)
            self.stats.candidates_saved += 1
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏–º—è –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–æ—Å—å)
            candidate_name = result.payload.name if result.payload else "Unknown"
            candidate_title = result.payload.title if result.payload else "Unknown Title"
            logger.info(f"   ‚úÖ Saved: {candidate_name} ({candidate_title})")
            
        except Exception as e:
            logger.error(f"   Failed to save result for {preview.resume_id}: {e}")

    def _check_page_safety(self, page_type: PageType, context: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–Ω–æ –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç—É.
        –ü—Ä–∏ BAN/CAPTCHA/LOGIN –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ–ª–∞–≥ critical_stop.
        """
        if page_type in [PageType.BAN, PageType.CAPTCHA, PageType.LOGIN]:
            logger.critical(f"üõë CRITICAL: Detected {page_type.value.upper()} on {context}. Stopping session.")
            self.stats.critical_stop = True
            self.stats.stop_reason = f"Blocked: {page_type.value}"
            return False
        return True