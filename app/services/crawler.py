import time
import logging
from typing import Optional, Dict, Any
from dataclasses import dataclass

from app.transport.fetcher import SmartFetcher
from app.parsing.base import BaseParser
from app.parsing.serp import SerpParser
from app.parsing.resume import ResumeParser
from app.parsing.models import PageType, DataQuality, ResumePreviewData
from app.storage.repository import JsonlRepository
from app.services.url_builder import UrlBuilder

logger = logging.getLogger(__name__)

@dataclass
class CrawlStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞."""
    pages_processed: int = 0
    candidates_found: int = 0
    candidates_new: int = 0
    candidates_saved: int = 0
    errors_serp: int = 0
    errors_detail: int = 0
    critical_stop: bool = False
    stop_reason: Optional[str] = None

class CrawlerService:
    """
    –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.
    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ü–∏–∫–ª: SERP -> Links -> Dedup -> Detail -> Save.
    –†–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –ø—Ä–æ–∫—Å–∏, –ø–æ—ç—Ç–æ–º—É —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–µ—Ç –∑–∞–¥–µ—Ä–∂–∫–∏ –∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –±–∞–Ω–µ.
    """

    # –ó–∞–¥–µ—Ä–∂–∫–∏ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö) –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞
    DELAY_SERP = 3.0    # –ú–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ø–∏—Å–∫–∞
    DELAY_DETAIL = 1.5  # –ú–µ–∂–¥—É —Ä–µ–∑—é–º–µ

    def __init__(self, fetcher: SmartFetcher, repository: JsonlRepository):
        self.fetcher = fetcher
        self.repository = repository
        self.stats = CrawlStats()

    def run(self, query: str, city: str = "", params: Dict[str, Any] = None, max_pages: int = 5) -> CrawlStats:
        """
        –ó–∞–ø—É—Å–∫ –∫—Ä–∞—É–ª–µ—Ä–∞ –ø–æ –ø–æ–∏—Å–∫–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É.
        """
        # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ URL
        start_url = UrlBuilder.build(query, city, params)
        logger.info(f"üöÄ Starting crawl. Query: '{query}', City: '{city}'. URL: {start_url}")
        
        current_url = start_url
        self.stats = CrawlStats()

        while current_url and self.stats.pages_processed < max_pages:
            if self.stats.critical_stop:
                break

            logger.info(f"üìÇ Processing SERP page {self.stats.pages_processed + 1}: {current_url}")
            
            # 2. –ó–∞–≥—Ä—É–∑–∫–∞ SERP
            # –ü–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–æ SERP, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä —Å–ø–∏—Å–∫–∞ –≤–Ω—É—Ç—Ä–∏ –ª–æ–≥–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            # –ù–æ —Å–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å HTML –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å PageType —á–µ—Ä–µ–∑ BaseParser (–≤–Ω—É—Ç—Ä–∏ fetcher)
            # SmartFetcher.get(url) –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä–æ–π HTML (bytes –∏–ª–∏ str). –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è PageType –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ BaseParser(html, url) –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ —Å–µ—Ä–≤–∏—Å–∞.
            
            try:
                html = self.fetcher.get(current_url)
            except Exception as e:
                logger.error(f"Network error fetching SERP: {e}")
                self.stats.errors_serp += 1
                break

            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ BaseParser (–∫–æ–Ω—Ç—Ä–∞–∫—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞, –∞ –Ω–µ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞)
            page_type = BaseParser(html, current_url).page_type

            # 3. Safety Checks
            if not self._check_page_safety(page_type, context="SERP"):
                break

            if page_type != PageType.SERP:
                logger.warning(f"Unexpected page type for SERP: {page_type}. Stopping.")
                self.stats.stop_reason = "Unexpected PageType"
                break

            # 4. –ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞
            # –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≤ SerpParser (–æ–Ω –Ω–∞—Å–ª–µ–¥—É–µ—Ç—Å—è –æ—Ç BaseParser, –Ω–æ –Ω–∞–º –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å 
            # –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞. SmartFetcher –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç BaseParser. 
            # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å–æ–∑–¥–∞—Ç—å SerpParser –∏–∑ —Å—ã—Ä–æ–≥–æ HTML, –∫–æ—Ç–æ—Ä—ã–π –µ—Å—Ç—å –≤ base_parser.soup, 
            # –Ω–æ SmartFetcher –Ω–µ —Ö—Ä–∞–Ω–∏—Ç raw bytes –ø—É–±–ª–∏—á–Ω–æ. 
            # –£–ø—Ä–æ—â–µ–Ω–∏–µ: SmartFetcher.get –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Å—Ç–∞–Ω—Å BaseParser. 
            # –ú—ã –ø–µ—Ä–µ—Å–æ–∑–¥–∞–¥–∏–º SerpParser, –ø–µ—Ä–µ–¥–∞–≤ soup.
            
            # –í–ê–ñ–ù–û: SerpParser –ø—Ä–∏–Ω–∏–º–∞–µ—Ç (html_content, url). 
            # –ß—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ, –±–µ—Ä–µ–º soup.encode() –∏–ª–∏ –ø–µ—Ä–µ–¥–∞–µ–º soup –Ω–∞–ø—Ä—è–º—É—é –µ—Å–ª–∏ –ø–∞—Ä—Å–µ—Ä –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç.
            # –ù–∞—à–∏ –ø–∞—Ä—Å–µ—Ä—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç bytes/str.
            serp_result = SerpParser(html, current_url).parse()

            if serp_result.quality == DataQuality.ERROR:
                logger.error("Failed to parse SERP structure.")
                self.stats.errors_serp += 1
                break

            # payload –¥–ª—è SERP - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ ResumePreviewData
            previews = serp_result.payload or []
            self.stats.candidates_found += len(previews)
            logger.info(f"   Found {len(previews)} candidates on page.")

            # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (Detail Loop)
            for preview in previews:
                if self.stats.critical_stop:
                    break
                
                self._process_candidate(preview)

            # 6. –ü–∞–≥–∏–Ω–∞—Ü–∏—è
            next_url = serp_result.next_page_url
            
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è
            if next_url == current_url:
                logger.warning("Next page URL matches current. Loop detected.")
                break
            
            current_url = next_url
            self.stats.pages_processed += 1

            if current_url:
                logger.debug(f"üí§ Sleeping {self.DELAY_SERP}s before next page...")
                time.sleep(self.DELAY_SERP)
        
        logger.info(f"üèÅ Crawl finished. Stats: {self.stats}")
        return self.stats

    def _process_candidate(self, preview: ResumePreviewData):
        """
        –õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è -> –°–∫–∞—á–∏–≤–∞–Ω–∏–µ -> –ü–∞—Ä—Å–∏–Ω–≥ -> –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ.
        """
        # 1. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (In-Memory check)
        if self.repository.exists(preview.resume_id):
            logger.debug(f"   Skipping existing ID: {preview.resume_id}")
            return

        self.stats.candidates_new += 1
        
        # 2. Throttling –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º –¥–µ—Ç–∞–ª–∫–∏
        time.sleep(self.DELAY_DETAIL)

        # 3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            html = self.fetcher.get(preview.url)
        except Exception as e:
            logger.error(f"   Failed to fetch detail {preview.url}: {e}")
            self.stats.errors_detail += 1
            return

        page_type = BaseParser(html, preview.url).page_type

        # 4. Safety Checks
        if page_type == PageType.NOT_FOUND:
            logger.warning(f"   Resume not found (404): {preview.url}")
            return

        if not self._check_page_safety(page_type, context="DETAIL"):
            return

        # 5. –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º raw html, –±–µ–∑ double-parsing)
        result = ResumeParser(html, preview.url).parse()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º (payload –∑–¥–µ—Å—å ResumeDetailData)
        if result.payload:
            self.repository.save_candidate(result.payload)
            self.stats.candidates_saved += 1
            logger.info(f"   ‚úÖ Saved: {result.payload.name} ({result.payload.title})")

    def _check_page_safety(self, page_type: PageType, context: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–Ω–æ –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç—É.
        –ü—Ä–∏ BAN/CAPTCHA/LOGIN –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ–ª–∞–≥ critical_stop.
        """
        if page_type in [PageType.BAN, PageType.CAPTCHA, PageType.LOGIN]:
            logger.critical(f"üõë CRITICAL: Detected {page_type.value.upper()} on {context}. Stopping session.")
            self.stats.critical_stop = True
            self.stats.stop_reason = f"Blocked: {page_type.value}"
            return False
        return True