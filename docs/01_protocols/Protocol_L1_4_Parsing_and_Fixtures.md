4: Парсинг HTML и Контракты Данных
Ниже зафиксирована финальная архитектура модуля парсинга, утвержденная после всех итераций и успешных тестов.

1. Контракты и Модели Данных
Файл: app/parsing/models.py
Здесь определена "Конституция" данных. Реализована строгая типизация через Pydantic.
Enums:
PageType: Классификация страницы (RESUME, SERP, LOGIN, CAPTCHA, BAN, NOT_FOUND).
DataQuality: Оценка результата (COMPLETE, PARTIAL, ERROR).
Payload DTOs (Полезная нагрузка):
ResumeDetailData: Полная анкета. Содержит вложенные DTO (SalaryDTO, ExperienceEntryDTO, EducationEntryDTO).
ResumePreviewData: Краткая карточка для списков.
Валидация: Реализованы жесткие валидаторы для resume_id (alphanumeric) и url (regex-паттерн Work.ua).
ParsingResult (Контейнер):
Обертка для результата. Разделяет метаданные (тип страницы, таймстамп) и чистые данные (payload).
Реализована полиморфная структура payload (может быть списком или объектом).
Гарантирует консистентность: запрещает наличие данных при ошибках доступа (BAN/CAPTCHA).

2. Реестр Селекторов (Конфигурация)
Файл: app/parsing/selectors.py
Централизованное хранилище CSS-путей, составленное на основе автоматического анализа реальных фикстур (analyze_structure.py).
Signatures: Селекторы для определения типа страницы (уникальные ID контейнеров, маркеры капчи).
SERP: Селекторы для списка карточек (учет классов .card и .card-visited).
Detail: Селекторы для полей резюме.
Фиксация: Для навыков закреплен точный селектор ul.list-unstyled.my-0.flex.flex-wrap span.ellipsis, соответствующий реальной DOM-структуре.

3. Базовый Движок (Core Engine)
Файл: app/parsing/base.py
Фундамент для всех парсеров.
Инициализация: Использует lxml для скоростного разбора HTML.
Классификация: Метод _classify_page автоматически определяет тип страницы (RESUME vs SERP vs BAN) по сигнатурам, исключая ложные срабатывания на общих страницах.
Очистка: Утилиты для нормализации текста (удаление \xa0, лишних пробелов).

4. Парсер Списка (SERP Parser)
Файл: app/parsing/serp.py
Отвечает за сбор ссылок для краулера.
Итерация: Обходит карточки внутри контейнера списка.
Абсолютные ссылки: Преобразует относительные пути (/resumes/123) в абсолютные с помощью urljoin.
Устойчивость: Пропускает битые карточки (реклама/баннеры), не ломая весь батч.

5. Парсер Резюме (Detail Parser)
Файл: app/parsing/resume.py
Самый сложный компонент, реализующий "хирургическое" извлечение данных.
Канонизация URL: Принудительно приводит URL к виду https://www.work.ua/resumes/{id}/, игнорируя UTM-метки и "грязные" ссылки на входе.
Изоляция (Scope): Парсинг происходит строго внутри контейнера div[id^='resume_'].
Секционный сканер:
Итерируется по заголовкам <h2>.
Определяет контекст ("Опыт", "Образование") по ключевым словам.
Извлекает контент между заголовками, не смешивая данные разных секций.
Игнорирует системные блоки ("Похожие кандидаты").
Сбор данных:
Опыт и Образование собираются в списки объектов.
Зарплата извлекается из шапки или блока характеристик с определением валюты.
Навыки извлекаются по уточненному селектору.

Итог: Модуль парсинга полностью готов, протестирован на реальных файлах и защищен от типовых ошибок (смещение данных, дублирование URL, мусор в полях).