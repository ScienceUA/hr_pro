import argparse
import asyncio
import json
import sys
import logging
from datetime import datetime, timezone
from pathlib import Path
from enum import Enum
from typing import Dict, Any, Optional

# –ò–º–ø–æ—Ä—Ç BS4 —Ç–µ–ø–µ—Ä—å —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, —Ç–∞–∫ –∫–∞–∫ –º—ã –µ–≥–æ —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏
from bs4 import BeautifulSoup
import httpx

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
sys.path.append(str(Path(__file__).resolve().parent.parent))

from app.config.settings import settings
from app.execution.http_client import HttpClientFactory
from app.execution.proxy_manager import proxy_manager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("fixture_fetcher")

# --- 0. DI Fix (Null Object Pattern) ---
class NullProxyManager:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è Direct-—Ä–µ–∂–∏–º–∞, —á—Ç–æ–±—ã —Ñ–∞–±—Ä–∏–∫–∞ –Ω–µ –ø–∞–¥–∞–ª–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ proxy_manager."""
    def get_next_proxy(self) -> Optional[str]:
        return None

# --- 1. Signature Contracts ---

class PageType(str, Enum):
    SERP = "serp"       # –°–ø–∏—Å–æ–∫ —Ä–µ–∑—é–º–µ
    RESUME = "resume"   # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞

class PageSignature(str, Enum):
    OK = "ok"                       # –í–∞–ª–∏–¥–Ω–∞—è —Ü–µ–ª–µ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
    PROTECTED = "protected"         # 403/401, –Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç –µ—Å—Ç—å (—Å–∫—Ä—ã—Ç—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã, –ª–æ–≥–∏–Ω)
    CAPTCHA = "captcha"             # –¢—Ä–µ–±—É–µ—Ç –≤–≤–æ–¥–∞ –∫–∞–ø—á–∏
    ACCESS_DENIED = "access_denied" # –ñ–µ—Å—Ç–∫–∏–π –±–∞–Ω (WAF)
    NOT_FOUND = "not_found"         # 404
    UNKNOWN = "unknown"             # 200, –Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –ø–æ—Ö–æ–∂–∞ –Ω–∞ —Ü–µ–ª–µ–≤—É—é

def classify_page(html: str, status_code: int, expected_type: PageType) -> PageSignature:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã. OK —Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    """
    soup = BeautifulSoup(html, "html.parser")
    lower_html = html.lower()

    # 1. Negative Checks (WAF / Captcha)
    if status_code in [403, 503] and ("cloudflare" in lower_html or "ray id" in lower_html):
        return PageSignature.ACCESS_DENIED
    
    if soup.select("#g-recaptcha-response") or soup.select("iframe[src*='captcha']"):
        return PageSignature.CAPTCHA

    # 2. Positive Checks (Structure Validation)
    if status_code == 200:
        if expected_type == PageType.RESUME:
            # –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–µ–∑—é–º–µ (H1, title)
            has_h1 = bool(soup.find("h1"))
            has_resume_marker = "—Ä–µ–∑—é–º–µ" in soup.title.text.lower() if soup.title else False
            
            if has_h1 or has_resume_marker:
                return PageSignature.OK
        
        elif expected_type == PageType.SERP:
            # –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–ø–∏—Å–∫–∞ (–∫–∞—Ä—Ç–æ—á–∫–∏)
            # –ò—â–µ–º div —Å –∫–ª–∞—Å—Å–æ–º card –∏–ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–∑—é–º–µ
            cards = soup.select("div.card") or soup.select("a[href*='/resumes/']")
            if cards:
                return PageSignature.OK
        
        return PageSignature.UNKNOWN

    # 3. Protected / Auth
    if status_code in [401, 403]:
        return PageSignature.PROTECTED

    if status_code == 404:
        return PageSignature.NOT_FOUND

    return PageSignature.UNKNOWN

# --- 2. Security & Sanitization ---

SAFE_HEADERS = {
    "content-type", "date", "server", "last-modified", "etag", "content-encoding", "vary"
}

def sanitize_headers(headers: httpx.Headers) -> Dict[str, str]:
    """–£–±–∏—Ä–∞–µ–º –∫—É–∫–∏ –∏ —Ç–æ–∫–µ–Ω—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤."""
    clean = {}
    for k, v in headers.items():
        if k.lower() in SAFE_HEADERS:
            clean[k.lower()] = v
    return clean

# --- 3. Fetch Logic ---

async def fetch_fixture(
    url: str, 
    name: str, 
    page_type: PageType,
    case_label: str,
    use_proxy: bool, 
    force: bool
):
    logger.info(f"üöÄ Starting fetch: {url}")
    
    # –í—ã–±–∏—Ä–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–∫—Å–∏
    pm = proxy_manager if use_proxy else NullProxyManager()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–±—Ä–∏–∫—É
    factory = HttpClientFactory(settings, pm)

    try:
        async with factory.client() as client:
            fetched_at = datetime.now(timezone.utc).isoformat()
            
            try:
                resp = await client.get(url)
            except Exception as e:
                logger.error(f"‚ùå Transport Error: {e}")
                return

            # --- Check 1: Content Type ---
            content_type = resp.headers.get("content-type", "").lower()
            if "text/html" not in content_type:
                logger.error(f"‚ùå Invalid Content-Type: {content_type}. Expected HTML.")
                if not force:
                    return

            # --- Check 2: Signature ---
            html_text = resp.text 
            signature = classify_page(html_text, resp.status_code, page_type)
            
            logger.info(f"üîç Signature: {signature.value.upper()} (Status: {resp.status_code})")

            # –†–∞–∑—Ä–µ—à–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–Ω—è—Ç–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –µ—Å–ª–∏ –Ω–µ—Ç --force
            valid_signatures = [PageSignature.OK, PageSignature.PROTECTED, PageSignature.NOT_FOUND]
            
            if signature not in valid_signatures and not force:
                logger.warning(f"‚ö†Ô∏è Signature '{signature.value}' rejected. Use --force to save anyway.")
                return

            # --- Saving ---
            base_dir = settings.BASE_DIR / "tests" / "fixtures" / "raw"
            base_dir.mkdir(parents=True, exist_ok=True)
            
            # –ò–º—è —Ñ–∞–π–ª–∞: type_case_name
            filename_base = f"{page_type.value}_{case_label}_{name}"
            html_path = base_dir / f"{filename_base}.html"
            meta_path = base_dir / f"{filename_base}.meta.json"

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –±–∏–Ω–∞—Ä–Ω–æ
            with open(html_path, "wb") as f:
                f.write(resp.content)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º Meta
            meta_data = {
                "url_requested": url,
                "url_final": str(resp.url),
                "status_code": resp.status_code,
                "fetched_at": fetched_at,
                "signature": signature.value,
                "type": page_type.value,
                "case": case_label,
                "fetch_mode": "proxy" if use_proxy else "direct",
                "content_type": content_type,
                "encoding_detected": resp.encoding or "utf-8",
                "headers": sanitize_headers(resp.headers)
            }
            
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(meta_data, f, indent=2, ensure_ascii=False)

            logger.info(f"‚úÖ Saved:\n  HTML: {html_path}\n  META: {meta_path}")

    except Exception as e:
        logger.exception(f"‚ùå Critical Error: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Smart Fixture Fetcher (L4.1)")
    
    parser.add_argument("--url", required=True)
    parser.add_argument("--name", required=True, help="Unique suffix")
    parser.add_argument("--type", required=True, choices=["serp", "resume"], help="Page type")
    parser.add_argument("--case", required=True, default="ok", help="Scenario label")
    parser.add_argument("--proxy", action="store_true")
    parser.add_argument("--force", action="store_true")
    
    args = parser.parse_args()
    
    asyncio.run(fetch_fixture(
        url=args.url,
        name=args.name,
        page_type=PageType(args.type),
        case_label=args.case,
        use_proxy=args.proxy,
        force=args.force
    ))