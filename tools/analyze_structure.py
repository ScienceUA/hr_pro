# tools/analyze_structure.py
import sys
import re
from pathlib import Path
from typing import Optional, Iterable, Tuple, List

from bs4 import BeautifulSoup, Tag

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (tools/ -> repo root)
PROJECT_ROOT = Path(__file__).resolve().parent.parent

# –í—Ö–æ–¥–Ω—ã–µ/–≤—ã—Ö–æ–¥–Ω—ã–µ –ø—É—Ç–∏
FIXTURES_DIR = PROJECT_ROOT / "tests" / "fixtures" / "raw"
REPORT_PATH = PROJECT_ROOT / "tests" / "fixtures" / "structure_report.txt"

# –¢–µ–≥–∏, –≤–Ω—É—Ç—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö —Ç–µ–∫—Å—Ç –Ω–∞–º –Ω–µ –Ω—É–∂–µ–Ω
SKIP_TEXT_PARENTS = {"script", "style", "meta", "noscript"}

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∑–∞—Ä–ø–ª–∞—Ç—ã
SALARY_KEYWORDS = ["–≥—Ä–Ω", "uah", "$", "usd", "‚Ç¨", "eur"]


def _safe_get_attr(tag: Tag, attr: str) -> Optional[str]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–π –∞—Ç—Ä–∏–±—É—Ç –∏–ª–∏ None."""
    val = tag.get(attr)
    if val is None:
        return None
    if isinstance(val, list):
        # class –æ–±—ã—á–Ω–æ list[str]
        return " ".join(str(x) for x in val if x)
    return str(val)


def _pick_stable_class(classes: List[str]) -> Optional[str]:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ "—Å—Ç–∞–±–∏–ª—å–Ω—ã–π" –∫–ª–∞—Å—Å:
    - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/–≥–µ–Ω–µ—Ä–∏–∫/—è–≤–Ω–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ
    - –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –∫–ª–∞—Å—Å—ã —Å –±—É–∫–≤–∞–º–∏, –±–µ–∑ –¥–ª–∏–Ω–Ω—ã—Ö —á–∏—Å–µ–ª
    """
    if not classes:
        return None

    def score(cls: str) -> Tuple[int, int]:
        # –º–µ–Ω—å—à–µ ‚Äî –ª—É—á—à–µ
        dynamic_penalty = 1 if re.search(r"\d{3,}", cls) else 0
        generic_penalty = 1 if cls in {"container", "row", "col", "card", "item"} else 0
        # –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –±–æ–ª–µ–µ "—Å–µ–º–∞–Ω—Ç–∏—á–Ω—ã–º" (–¥–ª–∏–Ω–Ω–µ–µ)
        length_penalty = -len(cls)
        return (dynamic_penalty + generic_penalty, length_penalty)

    filtered = []
    for c in classes:
        c = c.strip()
        if not c:
            continue
        if len(c) < 3:
            continue
        # –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –∑–∞–≤–µ–¥–æ–º–æ —Å–ª—É–∂–µ–±–Ω—ã–µ/–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if c.startswith(("js-", "is-", "has-")):
            continue
        filtered.append(c)

    if not filtered:
        return None

    filtered.sort(key=score)
    return filtered[0]


def build_stable_selector(tag: Tag) -> str:
    """
    –°—Ç—Ä–æ–∏—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞:
    –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: #id -> tag.stable_class -> tag
    """
    tag_name = tag.name if tag and tag.name else "*"

    tag_id = _safe_get_attr(tag, "id")
    if tag_id:
        # id –æ–±—ã—á–Ω–æ —Å–∞–º—ã–π —Å—Ç–∞–±–∏–ª—å–Ω—ã–π
        return f"{tag_name}#{tag_id}"

    cls_attr = tag.get("class") or []
    classes = [str(x) for x in cls_attr if x]
    stable_cls = _pick_stable_class(classes)
    if stable_cls:
        return f"{tag_name}.{stable_cls}"

    return tag_name


def build_selector_path(tag: Tag, max_depth: int = 4) -> str:
    """
    –°—Ç—Ä–æ–∏—Ç –ø—É—Ç—å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ "—Å–Ω–∏–∑—É –≤–≤–µ—Ä—Ö", –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è –≥–ª—É–±–∏–Ω—É.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ.
    """
    parts: List[str] = []
    cur = tag
    while cur and isinstance(cur, Tag) and cur.name != "[document]":
        parts.append(build_stable_selector(cur))
        cur = cur.parent  # type: ignore[assignment]
        if len(parts) >= max_depth:
            break
    parts.reverse()
    return " > ".join(parts)


def iter_text_nodes(soup: BeautifulSoup) -> Iterable[str]:
    """
    –ò—Ç–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –Ω–æ–¥—ã, –∏—Å–∫–ª—é—á–∞—è –º—É—Å–æ—Ä–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã.
    """
    for node in soup.find_all(string=True):
        text = str(node).strip()
        if not text:
            continue
        parent = getattr(node, "parent", None)
        if isinstance(parent, Tag) and parent.name in SKIP_TEXT_PARENTS:
            continue
        yield text


def analyze_one_file(filepath: Path, out) -> None:
    out.write(f"\n{'='*72}\nANALYZING: {filepath.name}\n{'='*72}\n")

    raw = filepath.read_bytes()

    try:
        soup = BeautifulSoup(raw, "lxml")
    except Exception as e:
        out.write(f"‚ùå PARSE ERROR (lxml): {e}\n")
        return

    # --- H1 ---
    h1 = soup.find("h1")
    if isinstance(h1, Tag):
        out.write("[H1] Found: True\n")
        out.write(f"     Path: {build_selector_path(h1)}\n")
        out.write(f"     Text: {h1.get_text(strip=True)[:80]}\n")
    else:
        out.write("[H1] Found: False\n")

    # --- H2 ---
    h2s = soup.find_all("h2")
    out.write(f"[H2] Headers count: {len(h2s)}\n")
    for i, h2 in enumerate(h2s[:30], start=1):  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –æ—Ç—á—ë—Ç–∞
        if not isinstance(h2, Tag):
            continue
        text = h2.get_text(strip=True)
        if not text:
            continue
        out.write(f"     #{i}: '{text[:80]}'\n")
        out.write(f"         Path: {build_selector_path(h2)}\n")

    # --- Salary candidates ---
    out.write("[SALARY] Candidates:\n")
    found_any = False
    for text in iter_text_nodes(soup):
        low = text.lower()
        if any(k in low for k in SALARY_KEYWORDS):
            # –Ω–∞–π–¥—ë–º –±–ª–∏–∂–∞–π—à–∏–π "—Å–º—ã—Å–ª–æ–≤–æ–π" —Ä–æ–¥–∏—Ç–µ–ª—å-—Ç–µ–≥
            # –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–¥–Ω—è—Ç—å—Å—è –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ –±–ª–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
            found_any = True
            # –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º Tag –∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫: –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π match –≤ –¥–µ—Ä–µ–≤–µ
            node = soup.find(string=lambda s: s and str(s).strip() == text)
            parent = getattr(node, "parent", None)
            if not isinstance(parent, Tag):
                continue

            # –ø–æ–¥–Ω–∏–º–∞–µ–º—Å—è –Ω–∞ 1-2 —É—Ä–æ–≤–Ω—è, –µ—Å–ª–∏ –ø–æ–ø–∞–ª–∏ –≤ span/i
            container = parent
            for _ in range(2):
                if isinstance(container, Tag) and container.name in {"span", "i", "b", "strong"} and isinstance(container.parent, Tag):
                    container = container.parent
            out.write(f"     Text: {text[:120]}\n")
            out.write(f"     Path: {build_selector_path(container)}\n")

    if not found_any:
        out.write("     NOT FOUND\n")

    # --- SERP links ---
    links = soup.select("a[href*='/resumes/']")
    # —Ñ–∏–ª—å—Ç—Ä: href –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å, –ø–æ—ç—Ç–æ–º—É –±–µ–∑–æ–ø–∞—Å–Ω–æ
    valid_links = []
    for a in links:
        if not isinstance(a, Tag):
            continue
        href = a.get("href")
        if not href:
            continue
        href_str = str(href)
        if "/resumes/" in href_str:
            valid_links.append(a)

    out.write(f"[SERP] Resume links count: {len(valid_links)}\n")
    if valid_links:
        sample = valid_links[0]
        out.write(f"     Sample HREF: {sample.get('href')}\n")
        out.write(f"     Sample Path: {build_selector_path(sample)}\n")

    # --- Generic "card-like" containers ---
    # –í–º–µ—Å—Ç–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è .card ‚Äî –∏—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–∑—é–º–µ
    out.write("[SERP] Card-like containers (first 5):\n")
    card_candidates = []
    for a in valid_links[:50]:
        # –ø–æ–¥–Ω–∏–º–µ–º—Å—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ div/section/article/li
        cur = a
        for _ in range(6):
            if isinstance(cur, Tag) and cur.name in {"div", "section", "article", "li"}:
                card_candidates.append(cur)
                break
            cur = cur.parent if isinstance(cur.parent, Tag) else None
            if cur is None:
                break

    # –¥–µ–¥—É–ø –ø–æ –ø—É—Ç–∏
    seen = set()
    shown = 0
    for c in card_candidates:
        path = build_selector_path(c)
        if path in seen:
            continue
        seen.add(path)
        out.write(f"     - {path}\n")
        shown += 1
        if shown >= 5:
            break

    if shown == 0:
        out.write("     (none)\n")


def main() -> None:
    if not FIXTURES_DIR.exists():
        print(f"‚ùå Fixtures directory not found: {FIXTURES_DIR}")
        print("   Ensure fixtures exist under tests/fixtures/raw/*.html")
        sys.exit(1)

    files = sorted(FIXTURES_DIR.glob("*.html"))
    if not files:
        print(f"‚ùå No .html files found in: {FIXTURES_DIR}")
        sys.exit(1)

    REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)

    print(f"üîç Analyzing {len(files)} fixture(s)...")
    with REPORT_PATH.open("w", encoding="utf-8") as out:
        out.write("STRUCTURE REPORT\n")
        out.write(f"Fixtures dir: {FIXTURES_DIR}\n")
        out.write(f"Count: {len(files)}\n")
        out.write("=" * 72 + "\n")
        for f in files:
            analyze_one_file(f, out)

    print("‚úÖ Analysis complete.")
    print(f"üìÑ Report saved to: {REPORT_PATH}")


if __name__ == "__main__":
    main()
